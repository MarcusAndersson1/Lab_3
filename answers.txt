/******************************************************************************
** DIT181  Datastrukturer och algoritmer, LP3 2021
** Lab 3: Plagiarism detection
*******************************************************************************/

Group members:
- [Jonatan Andersson]
- [Richard Novenius]
- [Marcus Andersson]

/******************************************************************************
** Task: Running the slow program
**
** 1. What is the asymptotic complexity of findSimilarity?
**    Answer in terms of N, the total number of 5-grams in the input files.
**    Assume that the number of duplicate occurrences of 5-grams is
**    a small constant - that is, there is not much plagiarised text.
******************************************************************************/


[ so n is the amount of ngrams in all documents to mathematicaly explain this we can say that
f1 {n = d * NgramInD}

for each document d1:         //  O(d)
     for each document d2 â‰  d1: //  O(d)
         for each 5-gram n1 in d1:  // O(NgramInD)
             for each 5-gram n2 in d2: // O(NgramInD)
                 if n1 = n2:  O(1)
                     increase the similarity of d1 and d2 by 1 O(1)


      tot O(d) * O(d) O(ngram) * O(ngram)*( O(1) + O(1)) remove constants
          O(d*ngram) * O(d*Ngram)  = O(n) * O(n) // according to f1
          O(n^2)

Answer:    O(n^2)]

/******************************************************************************
** 2. How long did the program take on the 'small' and 'medium' directories?
**    Is the ratio between the times what you would expect, given the complexity?
**    Explain very briefly why.
*******************************************************************************/

[ small took 3.29 sec and medium 317.14 sec calculating t2  = t1(n2/n1)^2  = 329 sec
 so 329 > 317.14 so comparing to the complexity it acts like expected.  ]

/******************************************************************************
** 3. How long do you predict the program would take to run on
**    the 'huge' directory? Show your calculations.
*******************************************************************************/

[
 n1 =  200 000, t1 = 317,14 sec
 n2 = 4 000 000, t2 = ?
 where n is input size and t is time.
 what is t2 ?
 t2  = t1(n2/n1)^2
 which results in 35h ]

/******************************************************************************
** Task: Using binary search trees
**
** 4. Which of the BSTs in the program usually become unbalanced?
**    Say very briefly how you deduced this.
******************************************************************************/

[index becomes unbalanced, we deduced since the size/ rebuild is the highest
  this means that the rebuild ratio is higher],
  without scapegoattree the index tree would become highly unbalanced]

/******************************************************************************
** 5 (optional). Is there a simple way to stop these trees becoming unbalanced?
******************************************************************************/

[...]

/******************************************************************************
** Task: Using scapegoat trees
**
** 6. Now what is the total asymptotic complexities of running and buildIndex
**    and findSimilarity? Include brief justification. Again, assume a total
**    of N 5-grams, and a constant number of duplicate occurrences of 5-grams.
******************************************************************************/

[

buildIndex

        for(Path path : files.keys()){                              // just needed to go through all paths doesnt
            for (Ngram ngram : files.get(path)) {                   // O(n) + log(n) the operation files get is a log(n) operation
                if(index.contains(ngram)){                          // O(log(n)
                    index.get(ngram).add(path);                     // O(log(n)) + O(log(n))
                }else{
                    ArrayList<Path> paths = new ArrayList<Path>();  // O(1)
                    paths.add(path);                                // O(1
                    index.put(ngram, paths);                        // O(log(n))
                }
            }
        }
        tot (O(n) + log(n)) * (O(log(n)) + O(log(n)) + O(log(n) + O(1) + O(1) + O(log(n)
        only consider highest complexity O(n) * O(log(n) = O(n*log(n))

findSimilarities
        for (Ngram ngram : index.keys()) {              // O(n)
            for (Path path1 : index.get(ngram)) {       //  since we assuming that the amount of similarities are small O(1)
                for (Path path2 : index.get(ngram)) {   //  since we assuming that the amount of similarities are small O(1)
                    if (path1.equals(path2))            // O(1)
                        continue;
                    PathPair pair = new PathPair(path1, path2);     // O(1)
                    if (!similarity.contains(pair))                 // O(log(n))
                    similarity.put(pair, 0);                        // O(log(n)
                    similarity.put(pair, similarity.get(pair) + 1); // O(log(n)) + O(log(n)
                }
            }
        }
        tot O(n) *(O(1) + O(1) + O(1) O(1) + O(log(n)) + O(log(n) + O(log(n)) + O(log(n) )
        only consider highest complexity O(n) * O(log(n) = O(n*log(n))


 Build index is O(nlog(n) due to that n is the amount of ngrams and to do this you have to iterate through all paths but you
 only visit every ngram once, then put has amortized O(log(n)) complexity
 and findSimilarities has O(n*log(n)) so total complexity is  nlog(n) + nlog(n) = O(nlog(n)

 Answer:  O(nlog(n) is the total asymptotic complexity]

/******************************************************************************
** 7 (optional). What if the total similarity score is an arbitrary number S,
**               rather than a small constant?
******************************************************************************/

[...]

/******************************************************************************
** Appendix: General information
**
** A. Approximately how many hours did you spend on the assignment?
******************************************************************************/

[Jonatan Andersson ]:  [15 h]
[Richard Novenius]:  [ 10h]
[Marcus Andersson]:  [ 6h ]

/******************************************************************************
** B. Are there any known bugs / limitations?
******************************************************************************/

[Not what we know but we barely made the ScapegoatTree work so have not tested everything]

/******************************************************************************
** C. Did you collaborate with any other students on this lab?
**    If so, please write in what way you collaborated and with whom.
**    Also include any resources (including the web) that you may
**    may have used in creating your design.
******************************************************************************/

[na....]

/******************************************************************************
** D. Describe any serious problems you encountered.                    
******************************************************************************/

[Yes we had serious problems getting the scapegoat tree to work ]

/******************************************************************************
** E. List any other comments here.
**    Feel free to provide any feedback on how much you learned 
**    from doing the assignment, and whether you enjoyed it.                                             
******************************************************************************/

[...]
